{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3613jvsc74a57bd0f14d0265c58dcca937431964c116f2c2daf05ac4f2d61c58d7f022c31778fcd5",
   "display_name": "Python 3.6.13 64-bit ('torch': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "f14d0265c58dcca937431964c116f2c2daf05ac4f2d61c58d7f022c31778fcd5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 0 DataLoader for FashionMNIST"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    # compose transforms.Resize() and transforms.ToTensor() together\n",
    "    trans = []\n",
    "    if resize:\n",
    "        trans.append(transforms.Resize(size=resize))\n",
    "    trans.append(transforms.ToTensor())\n",
    "    # compose transforms.Resize() and transforms.ToTensor() together\n",
    "    transform = transforms.Compose(trans)\n",
    "    train_data = torchvision.datasets.FashionMNIST(root = \"./data/FashionMNIST\", train=True, transform=transform, download=True)\n",
    "    test_data = torchvision.datasets.FashionMNIST(root = \"./data/FashionMNIST\", train=False, transform=transform, download=True)\n",
    "\n",
    "    train_iter = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    test_iter = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "source": [
    "# 1 AlexNet(simplified)\n",
    "1. AlexNet uses ReLU insted of sigmoid\n",
    "2. AlexNet uses Dropout"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            #1*227*227->96*55*55\n",
    "            nn.Conv2d(in_channels=1, out_channels=96, kernel_size=11, stride=4),\n",
    "            nn.ReLU(),\n",
    "            #96*55*55->96*27*27\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            #96*27*27->256*27*27\n",
    "            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            #256*27*27->256*13*13\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            #256*13*13->384*13*13\n",
    "            nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            #384*13*13->384*13*13\n",
    "            nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            #384*13*13->256*13*13\n",
    "            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            #256*13*13->256*6*6\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256*6*6, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        feature = self.conv(img)\n",
    "        output = self.fc(feature.view(img.shape[0], -1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AlexNet(\n  (conv): Sequential(\n    (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU()\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU()\n    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU()\n    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU()\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (fc): Sequential(\n    (0): Linear(in_features=9216, out_features=4096, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=10, bias=True)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "net = AlexNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size, resize=227)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        # Use the device net is on\n",
    "        device = list(net.parameters())[0].device\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            net.eval() # close Dropout\n",
    "            y_hat = net(X.to(device))\n",
    "            acc_sum += (y_hat.argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            net.train() # reuse Dropout\n",
    "    return acc_sum / n\n",
    "\n",
    "def train(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs):\n",
    "    net = net.to(device)\n",
    "    print(\" training on \", device)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            # We do not use to store them on GPU\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.001, 2\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "#train(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "source": [
    "# 2 Network in Network(NiN)\n",
    "Typically we extract features exploiting **spatial structure** via a sequence of convolution and pooling layers and then post-process the representations via FC layers. A careless use of FC layers might give up the spacial structure.\n",
    "\n",
    "Alternatively NiN offers an equivalent way to use FC layers earlier without giving up the spacial structure.\n",
    "\n",
    "The NiN block consists of one (user-defined) convolutional layer followed by two 1*1 convolutional layers that act as per-pixel fully-connected layers with ReLU activations.\n",
    "\n",
    "**If we see a whole channel as a feature, 1*1 convolutional layer is like to perform a FC layer between channels**\n",
    "\n",
    "NiN avoids overfitting by preventing FC layers with too many parameters\n",
    "## 2.1 NiN block"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nin_block(in_channels, out_channels, kernel_size, stride, padding):\n",
    "    blk = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "    return blk"
   ]
  },
  {
   "source": [
    "## 2.2 NiN version of AlexNet\n",
    "Instead of using FC layers at the end of our network, we use global average Pooling layer to reshape the output"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GlobalAvgPool2d(nn.Module):\n",
    "    \"\"\"\n",
    "    input: b * c * h * w\n",
    "\n",
    "    output: b * c * 1 * 1\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "    def forward(self, x):\n",
    "        # x.size():[batch_size, channels, height, width]\n",
    "        return F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "\n",
    "class FlattenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlattenLayer, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    # 1*227*227->96*55*55\n",
    "    nin_block(in_channels=1, out_channels=96, kernel_size=11, stride=4, padding=0),\n",
    "    # 96*55*55->96*27*27\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    # 96*27*27->256*27*27\n",
    "    nin_block(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2),\n",
    "    # 256*27*27->256*13*13\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    # 256*13*13->384*13*13\n",
    "    nin_block(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
    "    # 384*13*13->384*6*6\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nn.Dropout(0.5),\n",
    "    # 384*6*6->10*6*6\n",
    "    nin_block(in_channels=384, out_channels=10, kernel_size=3, stride=1,padding=1),\n",
    "    # 10*6*6->10*1*1\n",
    "    GlobalAvgPool2d(),\n",
    "    # 10*1*1->10\n",
    "    FlattenLayer()\n",
    ")"
   ]
  },
  {
   "source": [
    "## 2.3 Output shape of NiN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 output shape:  torch.Size([1, 96, 55, 55])\n1 output shape:  torch.Size([1, 96, 27, 27])\n2 output shape:  torch.Size([1, 256, 27, 27])\n3 output shape:  torch.Size([1, 256, 13, 13])\n4 output shape:  torch.Size([1, 384, 13, 13])\n5 output shape:  torch.Size([1, 384, 6, 6])\n6 output shape:  torch.Size([1, 384, 6, 6])\n7 output shape:  torch.Size([1, 10, 6, 6])\n8 output shape:  torch.Size([1, 10, 1, 1])\n9 output shape:  torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 1, 227, 227)\n",
    "for name, blk in net.named_children():\n",
    "    X = blk(X)\n",
    "    print(name, \"output shape: \", X.shape)"
   ]
  },
  {
   "source": [
    "## 2.4 Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "lr, num_epochs = 0.001, 2\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size, resize=227)\n",
    "\n",
    "#train(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "source": [
    "# 3 Batch Normalization\n",
    "Batch Normalization layer should be between FC/Conv layer and activation function layer. Each channel has its own BatchNormalization layer.\n",
    "\n",
    "Assume the input and output of Batch Normalization layer as $\\boldsymbol{x},\\ \\boldsymbol{y}$: $\\boldsymbol{y} = BN(\\boldsymbol{x})$\n",
    "\n",
    "Process:\n",
    "- Calculate mean and variance of a batch:\n",
    "$$\\boldsymbol{\\mu}_\\mathcal{B} \\leftarrow \\frac{1}{m}\\sum_{i = 1}^{m} \\boldsymbol{x}^{(i)},$$\n",
    "$$\\boldsymbol{\\sigma}_\\mathcal{B}^2 \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m}(\\boldsymbol{x}^{(i)} - \\boldsymbol{\\mu}_\\mathcal{B})^2,$$\n",
    "- Normalization\n",
    "$$\\hat{\\boldsymbol{x}}^{(i)} \\leftarrow \\frac{\\boldsymbol{x}^{(i)} - \\boldsymbol{\\mu}_\\mathcal{B}}{\\sqrt{\\boldsymbol{\\sigma}_\\mathcal{B}^2 + \\epsilon}},$$\n",
    "- scale and shift\n",
    "$${\\boldsymbol{y}}^{(i)} \\leftarrow \\boldsymbol{\\gamma} \\odot \\hat{\\boldsymbol{x}}^{(i)} + \\boldsymbol{\\beta}.$$\n",
    "\n",
    "$\\boldsymbol{\\gamma}$ and $\\boldsymbol{\\beta}$ are learnable parameters\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet with Batch Normalization\n",
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class FlattenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlattenLayer, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    # 1*28*28->6*24*24\n",
    "    nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
    "    # 6*24*24->6*24*24\n",
    "    nn.BatchNorm2d(num_features=6),\n",
    "    # 6*24*24->6*24*24\n",
    "    nn.Sigmoid(),\n",
    "    # 6*24*24->6*12*12\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    # 6*12*12->16*8*8\n",
    "    nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "    # 16*8*8->16*8*8\n",
    "    nn.BatchNorm2d(num_features=16),\n",
    "    # 16*8*8->16*8*8\n",
    "    nn.Sigmoid(),\n",
    "    # 16*8*8->16*4*4\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    FlattenLayer(),\n",
    "    nn.Linear(16*4*4, 120),\n",
    "    nn.BatchNorm1d(120),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.BatchNorm1d(84),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(84, 10)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size)\n",
    "\n",
    "lr, num_epochs = 0.001, 2\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "#train(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "source": [
    "# 4 Residual Networks(ResNet)\n",
    "Instead of directly leaning the mapping $f(x)$, we learn the **residual mapping** $f(x)-x$, which is easier to learn.\n",
    "\n",
    "Note: $x$ and $f(x)$ have to be in the same shape. The only exception is when $x$ and $f(x)$ have different amount of channels. Then we need to use 1*1 convolutional layer to adjust the channel amount of $x$.\n",
    "\n",
    "## 4.1 ResNet Block\n",
    "We assume Residual Layer as \n",
    "\n",
    "x->\n",
    "\n",
    "Conv->BN->ReLU->\n",
    "\n",
    "Conv->BN->ReLU->\n",
    "\n",
    "1*1 Conv->ReLU\n",
    "\n",
    "->f(x)-x"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_1x1conv=False, stride=1):\n",
    "        super(ResidualLayer, self).__init__()\n",
    "        # h*h->([h-1]/s+1) * ([h-1]/s+1)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        # ([h-1]/s+1) * ([h-1]/s+1)->([h-1]/s+1) * ([h-1]/s+1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            # h*h->([h-1]/s+1) * ([h-1]/s+1)\n",
    "            self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        if self.conv3:\n",
    "            x = self.conv3(x)\n",
    "        # learn f(x) - x\n",
    "        return F.relu(x + y)"
   ]
  },
  {
   "source": [
    "## 4.2 ResNet Model\n",
    "Our Model consists of several ResNet Modules and each ResNet Module consists of several (2 by default) ResNet Layers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First part\n",
    "net = nn.Sequential(\n",
    "        nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "        nn.BatchNorm2d(64), \n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_block(in_channels, out_channels, num_residuals=2):\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0:\n",
    "            blk.append(ResidualLayer(in_channels, out_channels, use_1x1conv=True, stride=2))\n",
    "        else:\n",
    "            blk.append(ResidualLayer(out_channels, out_channels))\n",
    "    return nn.Sequential(*blk)\n",
    "\n",
    "# Second part: Residual Model\n",
    "net.add_module(\"resnet_block1\", resnet_block(64, 64, 2))\n",
    "net.add_module(\"resnet_block2\", resnet_block(64, 128, 2))\n",
    "net.add_module(\"resnet_block3\", resnet_block(128, 256, 2))\n",
    "net.add_module(\"resnet_block4\", resnet_block(256, 512, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAvgPool2d(nn.Module):\n",
    "    \"\"\"\n",
    "    input: b * c * h * w\n",
    "\n",
    "    output: b * c * 1 * 1\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "    def forward(self, x):\n",
    "        # x.size():[batch_size, channels, height, width]\n",
    "        return F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "\n",
    "class FlattenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlattenLayer, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "# Third part: global average Pooling and FC\n",
    "net.add_module(\"global_avg_pool\", GlobalAvgPool2d()) \n",
    "net.add_module(\"fc\", nn.Sequential(FlattenLayer(), nn.Linear(512, 10))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0  output shape:\t torch.Size([1, 64, 114, 114])\n1  output shape:\t torch.Size([1, 64, 114, 114])\n2  output shape:\t torch.Size([1, 64, 114, 114])\n3  output shape:\t torch.Size([1, 64, 57, 57])\nresnet_block1  output shape:\t torch.Size([1, 64, 29, 29])\nresnet_block2  output shape:\t torch.Size([1, 128, 15, 15])\nresnet_block3  output shape:\t torch.Size([1, 256, 8, 8])\nresnet_block4  output shape:\t torch.Size([1, 512, 4, 4])\nglobal_avg_pool  output shape:\t torch.Size([1, 512, 1, 1])\nfc  output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand((1, 1, 227, 227))\n",
    "for name, layer in net.named_children():\n",
    "    X = layer(X)\n",
    "    print(name, ' output shape:\\t', X.shape)"
   ]
  },
  {
   "source": [
    "# 5 Densely Connected Networks (DenseNet)\n",
    "Instead of adding two layers together in ResNet, we **concatenate** two layers towards a new layer\n",
    "\n",
    "We assume a ConvBlock as the combination of BN, ReLU and Convã€‚\n",
    "\n",
    "A Dense block can be depicted as\n",
    "\n",
    "x -> ConvBlock -> \\[x, ConvBlock(x)\\] -> ...\n",
    "\n",
    "## 5.1 Dense Block\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvBlock(in_channels, out_channels):\n",
    "    blk = nn.Sequential(\n",
    "        nn.BatchNorm2d(in_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "    )\n",
    "    return blk\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, num_convs, in_channels, out_channels):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        net = []\n",
    "        in_c = in_channels\n",
    "        for i in range(num_convs):\n",
    "            net.append(ConvBlock(in_c, out_channels))\n",
    "            # in_channels of next block should be the amount of channels of concatenated output of last block\n",
    "            in_c += out_channels\n",
    "        # Use ModuleList to avoid automatic forward()\n",
    "        self.net = nn.ModuleList(net)\n",
    "        # out_channels of the last block after concatenation\n",
    "        self.out_channels = in_channels + num_convs * out_channels\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for blk in self.net:\n",
    "            y = blk(x)\n",
    "            # dim=1 means concatenation in channel dimension\n",
    "            x = torch.cat((x, y), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 23, 8, 8])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "blk = DenseBlock(2, 3, 10)\n",
    "x = torch.rand(4, 3, 8, 8)\n",
    "y = blk(x)\n",
    "y.shape # 2 + 3 * 10 = 23"
   ]
  },
  {
   "source": [
    "## 5.2 Transition Block\n",
    "Transition Block is used to avoid accumulation of channels using 1*1 Convolutional layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 4, 4])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "def TransitionBlock(in_channels, out_channels):\n",
    "    blk = nn.Sequential(\n",
    "        nn.BatchNorm2d(in_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "        # different from self-defined GlobalAvgPool2d\n",
    "        nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "    )\n",
    "    return blk\n",
    "\n",
    "blk = TransitionBlock(23, 10)\n",
    "blk(y).shape"
   ]
  },
  {
   "source": [
    "## 5.3 DesNet Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Part\n",
    "net = nn.Sequential(\n",
    "        nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "        nn.BatchNorm2d(64), \n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Part\n",
    "# num_channels: the current amount of output channels of the first part\n",
    "# growth_rate: also the output of Conv Block\n",
    "num_channels, growth_rate = 64, 32  \n",
    "num_convs_in_dense_blocks = [4, 4, 4, 4]\n",
    "\n",
    "for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "    DB = DenseBlock(num_convs, num_channels, growth_rate)\n",
    "    net.add_module(\"DenseBlosk_%d\" % i, DB)\n",
    "    num_channels = DB.out_channels\n",
    "    # Add transition block that cut off half of the channels\n",
    "    if i != len(num_convs_in_dense_blocks) - 1:\n",
    "        net.add_module(\"TransitionBlock_%d\" % i, TransitionBlock(num_channels, num_channels // 2))\n",
    "        num_channels = num_channels // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAvgPool2d(nn.Module):\n",
    "    \"\"\"\n",
    "    input: b * c * h * w\n",
    "\n",
    "    output: b * c * 1 * 1\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "    def forward(self, x):\n",
    "        # x.size():[batch_size, channels, height, width]\n",
    "        return F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "\n",
    "class FlattenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlattenLayer, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "# Third Part:\n",
    "net.add_module(\"BN\", nn.BatchNorm2d(num_channels))\n",
    "net.add_module(\"ReLU\", nn.ReLU())\n",
    "net.add_module(\"AvgPool\", GlobalAvgPool2d()) \n",
    "net.add_module(\"FC\", nn.Sequential(FlattenLayer(), nn.Linear(num_channels, 10)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0  output shape:\t torch.Size([1, 64, 48, 48])\n1  output shape:\t torch.Size([1, 64, 48, 48])\n2  output shape:\t torch.Size([1, 64, 48, 48])\n3  output shape:\t torch.Size([1, 64, 24, 24])\nDenseBlosk_0  output shape:\t torch.Size([1, 192, 24, 24])\nTransitionBlock_0  output shape:\t torch.Size([1, 96, 12, 12])\nDenseBlosk_1  output shape:\t torch.Size([1, 224, 12, 12])\nTransitionBlock_1  output shape:\t torch.Size([1, 112, 6, 6])\nDenseBlosk_2  output shape:\t torch.Size([1, 240, 6, 6])\nTransitionBlock_2  output shape:\t torch.Size([1, 120, 3, 3])\nDenseBlosk_3  output shape:\t torch.Size([1, 248, 3, 3])\nBN  output shape:\t torch.Size([1, 248, 3, 3])\nReLU  output shape:\t torch.Size([1, 248, 3, 3])\nAvgPool  output shape:\t torch.Size([1, 248, 1, 1])\nFC  output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand((1, 1, 96, 96))\n",
    "for name, layer in net.named_children():\n",
    "    X = layer(X)\n",
    "    print(name, ' output shape:\\t', X.shape)"
   ]
  }
 ]
}